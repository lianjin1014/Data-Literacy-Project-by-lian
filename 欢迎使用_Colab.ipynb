{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lianjin1014/Data-Literacy-Project-by-lian/blob/main/%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (roc_auc_score, accuracy_score, precision_score,\n",
        "                             recall_score, f1_score, confusion_matrix, roc_curve)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.over_sampling import SMOTE\n",
        "DATA_DIR = \".\"\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE    = 0.2\n",
        "\n",
        "MAX_ROWS     = 120_000    # LR / RF / XGB 的训练上限\n",
        "SVM_MAX_ROWS = 25_000     # SVM 更耗时，用更小上限\n",
        "\n",
        "# 是否对线性/核方法启用类不平衡权重\n",
        "USE_CLASS_WEIGHT_BALANCED = True"
      ],
      "metadata": {
        "id": "Km7A8p-oDUGk"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- 路径 --------------------\n",
        "TRAIN_TRANS_PATH = os.path.join(DATA_DIR, \"train_transaction.csv\")\n",
        "TRAIN_ID_PATH    = os.path.join(DATA_DIR, \"train_identity.csv\")\n",
        "TEST_TRANS_PATH  = os.path.join(DATA_DIR, \"test_transaction.csv\")\n",
        "TEST_ID_PATH     = os.path.join(DATA_DIR, \"test_identity.csv\")\n",
        "SAMPLE_SUB_PATH  = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
        "\n",
        "# -------------------- 输出 --------------------\n",
        "OUT_METRICS_CSV  = os.path.join(DATA_DIR, \"model_metrics.csv\")\n",
        "OUT_ROC_PNG      = os.path.join(DATA_DIR, \"roc_curves.png\")\n",
        "OUT_CM_PNG       = os.path.join(DATA_DIR, \"confusion_matrix_best.png\")\n",
        "OUT_SUBMISSION   = os.path.join(DATA_DIR, \"submission_best_model.csv\")\n"
      ],
      "metadata": {
        "id": "o7Nr67_4DUDy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_available = True\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "except Exception as e:\n",
        "    xgb_available = False\n",
        "    xgb_import_error = str(e)\n",
        "\n",
        "def load_and_merge(trans_path, id_path):\n",
        "    trans = pd.read_csv(trans_path)\n",
        "    ident = pd.read_csv(id_path)\n",
        "    return trans.merge(ident, on=\"TransactionID\", how=\"left\")\n",
        "\n",
        "def stratified_cap(X, y, cap, seed=RANDOM_STATE):\n",
        "    if (cap is None) or (len(X) <= cap):\n",
        "        return X, y\n",
        "    Xs, _, ys, _ = train_test_split(X, y, train_size=cap, stratify=y, random_state=seed)\n",
        "    return Xs, ys\n",
        "\n",
        "def align_columns_for_inference(X_df, num_cols, cat_cols):\n",
        "    X = X_df.copy()\n",
        "    wanted = list(num_cols) + list(cat_cols)\n",
        "\n",
        "    # add missing\n",
        "    for c in wanted:\n",
        "        if c not in X.columns:\n",
        "            if c in num_cols:\n",
        "                X[c] = np.nan\n",
        "            else:\n",
        "                X[c] = pd.Series([np.nan] * len(X), dtype=\"object\")\n",
        "\n",
        "    # drop extras & reorder\n",
        "    X = X[wanted]\n",
        "    return X\n",
        "\n",
        "def evaluate_model(name, pipe, Xtr, ytr, Xva, yva, store_curves):\n",
        "    pipe.fit(Xtr, ytr)\n",
        "    if hasattr(pipe, \"predict_proba\"):\n",
        "        proba = pipe.predict_proba(Xva)[:, 1]\n",
        "    elif hasattr(pipe, \"decision_function\"):\n",
        "        dfun  = pipe.decision_function(Xva)\n",
        "        proba = (dfun - dfun.min()) / (dfun.max() - dfun.min() + 1e-9)\n",
        "    else:\n",
        "        proba = pipe.predict(Xva)\n",
        "\n",
        "    preds = (proba >= 0.5).astype(int)\n",
        "\n",
        "    auc  = roc_auc_score(yva, proba)\n",
        "    acc  = accuracy_score(yva, preds)\n",
        "    prec = precision_score(yva, preds, zero_division=0)\n",
        "    rec  = recall_score(yva, preds, zero_division=0)\n",
        "    f1   = f1_score(yva, preds, zero_division=0)\n",
        "    cm   = confusion_matrix(yva, preds)\n",
        "    fpr, tpr, _ = roc_curve(yva, proba)\n",
        "    store_curves[name] = (fpr, tpr, auc)\n",
        "\n",
        "    return {\"model\": name, \"ROC_AUC\": auc, \"Accuracy\": acc,\n",
        "            \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"cm\": cm, \"pipe\": pipe}\n"
      ],
      "metadata": {
        "id": "-11u_3yzDUAo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81a14854"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "def build_preprocessing_pipeline(num_imputation_strategy=\"median\", cat_encoding_strategy=\"ordinal\", numerical_scaling=True, num_cols=None, cat_cols=None):\n",
        "    \"\"\"\n",
        "    Builds a preprocessing pipeline based on specified strategies.\n",
        "\n",
        "    Args:\n",
        "        num_imputation_strategy (str): Strategy for numerical imputation ('mean' or 'median').\n",
        "        cat_encoding_strategy (str): Strategy for categorical encoding ('ordinal' or 'onehot').\n",
        "        numerical_scaling (bool): Whether to apply StandardScaler to numerical features.\n",
        "        num_cols (list): List of numerical column names.\n",
        "        cat_cols (list): List of categorical column names.\n",
        "\n",
        "    Returns:\n",
        "        ColumnTransformer: The constructed preprocessing pipeline.\n",
        "    \"\"\"\n",
        "    if num_cols is None or cat_cols is None:\n",
        "        raise ValueError(\"num_cols and cat_cols must be provided.\")\n",
        "\n",
        "    # Numerical pipeline\n",
        "    num_imputer = SimpleImputer(strategy=num_imputation_strategy)\n",
        "    if numerical_scaling:\n",
        "        num_pipeline = Pipeline([\n",
        "            (\"imputer\", num_imputer),\n",
        "            (\"scaler\", StandardScaler()),\n",
        "        ])\n",
        "    else:\n",
        "        num_pipeline = Pipeline([\n",
        "            (\"imputer\", num_imputer),\n",
        "        ])\n",
        "\n",
        "    # Categorical pipeline\n",
        "    cat_imputer = SimpleImputer(strategy=\"most_frequent\") # Often good default for categorical\n",
        "    if cat_encoding_strategy == \"ordinal\":\n",
        "        cat_encoder = OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)\n",
        "    elif cat_encoding_strategy == \"onehot\":\n",
        "        cat_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False) # Use sparse_output=False for dense output\n",
        "    else:\n",
        "        raise ValueError(\"Invalid cat_encoding_strategy. Use 'ordinal' or 'onehot'.\")\n",
        "\n",
        "    cat_pipeline = Pipeline([\n",
        "        (\"imputer\", cat_imputer),\n",
        "        (\"encoder\", cat_encoder),\n",
        "    ])\n",
        "\n",
        "    # Column Transformer\n",
        "    preprocessor = ColumnTransformer([\n",
        "        (\"num\", num_pipeline, num_cols),\n",
        "        (\"cat\", cat_pipeline, cat_cols),\n",
        "    ], remainder=\"drop\")\n",
        "\n",
        "    return preprocessor"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eab3a7f"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the SMOTE class and update the main function to include the SMOTE application based on a flag.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOCTMO2HUYJN",
        "outputId": "cb1fb75a-2894-48f1-e950-ffc4b05711b9"
      },
      "source": [
        "# Update the DATA_DIR variable to the correct path where the data files are located\n",
        "DATA_DIR = \"/content/\"\n",
        "USE_SMOTE = True\n",
        "def main():\n",
        "    t0 = time.time()\n",
        "    print(\"Loading & merging...\")\n",
        "\n",
        "    # Redefine file paths using the updated DATA_DIR\n",
        "    TRAIN_TRANS_PATH = os.path.join(DATA_DIR, \"train_transaction.csv\")\n",
        "    TRAIN_ID_PATH    = os.path.join(DATA_DIR, \"train_identity.csv\")\n",
        "    TEST_TRANS_PATH  = os.path.join(DATA_DIR, \"test_transaction.csv\")\n",
        "    TEST_ID_PATH     = os.path.join(DATA_DIR, \"test_identity.csv\")\n",
        "    SAMPLE_SUB_PATH  = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
        "\n",
        "    # -------------------- Output Paths --------------------\n",
        "    OUT_METRICS_CSV  = os.path.join(DATA_DIR, \"model_metrics.csv\")\n",
        "    OUT_ROC_PNG      = os.path.join(DATA_DIR, \"roc_curves.png\")\n",
        "    OUT_CM_PNG       = os.path.join(DATA_DIR, \"confusion_matrix_best.png\")\n",
        "    OUT_SUBMISSION   = os.path.join(DATA_DIR, \"submission_best_model.csv\")\n",
        "\n",
        "\n",
        "    train = load_and_merge(TRAIN_TRANS_PATH, TRAIN_ID_PATH)\n",
        "    test  = load_and_merge(TEST_TRANS_PATH, TEST_ID_PATH)\n",
        "\n",
        "    assert \"isFraud\" in train.columns, \"训练集必须包含目标列 isFraud\"\n",
        "    id_col = \"TransactionID\"\n",
        "\n",
        "    y_full = train[\"isFraud\"].astype(int)\n",
        "    X_full = train.drop(columns=[\"isFraud\"])\n",
        "    assert id_col in X_full.columns and id_col in test.columns\n",
        "\n",
        "    #分层抽样（整体）\n",
        "    X_cap, y_cap = stratified_cap(X_full, y_full, MAX_ROWS)\n",
        "\n",
        "    # 训练/验证切分\n",
        "    X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "        X_cap, y_cap, test_size=TEST_SIZE, stratify=y_cap, random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # 列类型识别\n",
        "    num_cols = X_tr.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if id_col in num_cols: num_cols.remove(id_col)\n",
        "    cat_cols = [c for c in X_tr.columns if c not in num_cols and c != id_col]\n",
        "\n",
        "    # Define preprocessing pipelines using the new function\n",
        "    preproc_linear = build_preprocessing_pipeline(\n",
        "        num_imputation_strategy=\"median\",\n",
        "        cat_encoding_strategy=\"ordinal\",\n",
        "        numerical_scaling=True,\n",
        "        num_cols=num_cols,\n",
        "        cat_cols=cat_cols\n",
        "    )\n",
        "    preproc_tree = build_preprocessing_pipeline(\n",
        "        num_imputation_strategy=\"median\",\n",
        "        cat_encoding_strategy=\"ordinal\",\n",
        "        numerical_scaling=False, # Tree models don't need scaling\n",
        "        num_cols=num_cols,\n",
        "        cat_cols=cat_cols\n",
        "    )\n",
        "\n",
        "\n",
        "    # Apply preprocessing and SMOTE if needed\n",
        "    X_tr_processed_linear = preproc_linear.fit_transform(X_tr.drop(columns=[id_col], errors=\"ignore\"))\n",
        "    X_va_processed_linear = preproc_linear.transform(X_va.drop(columns=[id_col], errors=\"ignore\"))\n",
        "\n",
        "    X_tr_processed_tree = preproc_tree.fit_transform(X_tr.drop(columns=[id_col], errors=\"ignore\"))\n",
        "    X_va_processed_tree = preproc_tree.transform(X_va.drop(columns=[id_col], errors=\"ignore\"))\n",
        "\n",
        "    if USE_SMOTE:\n",
        "        print(\"Applying SMOTE...\")\n",
        "        smote = SMOTE(random_state=RANDOM_STATE)\n",
        "        # SMOTE works on numerical data after preprocessing\n",
        "        # Need to preprocess X_tr *before* SMOTE\n",
        "        # Let's define preprocessing pipelines first\n",
        "\n",
        "    # Define preprocessing pipelines using the new function\n",
        "    preproc_linear = build_preprocessing_pipeline(\n",
        "        num_imputation_strategy=\"median\",\n",
        "        cat_encoding_strategy=\"ordinal\",\n",
        "        numerical_scaling=True,\n",
        "        num_cols=num_cols,\n",
        "        cat_cols=cat_cols\n",
        "    )\n",
        "    preproc_tree = build_preprocessing_pipeline(\n",
        "        num_imputation_strategy=\"median\",\n",
        "        cat_encoding_strategy=\"ordinal\",\n",
        "        numerical_scaling=False, # Tree models don't need scaling\n",
        "        num_cols=num_cols,\n",
        "        cat_cols=cat_cols\n",
        "    )\n",
        "\n",
        "\n",
        "    # Apply preprocessing and SMOTE if needed\n",
        "    X_tr_processed_linear = preproc_linear.fit_transform(X_tr.drop(columns=[id_col], errors=\"ignore\"))\n",
        "    X_va_processed_linear = preproc_linear.transform(X_va.drop(columns=[id_col], errors=\"ignore\"))\n",
        "\n",
        "    X_tr_processed_tree = preproc_tree.fit_transform(X_tr.drop(columns=[id_col], errors=\"ignore\"))\n",
        "    X_va_processed_tree = preproc_tree.transform(X_va.drop(columns=[id_col], errors=\"ignore\"))\n",
        "\n",
        "    if USE_SMOTE:\n",
        "        print(\"Applying SMOTE to preprocessed tree data...\")\n",
        "        smote = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_res_tree, y_res_tree = smote.fit_resample(X_tr_processed_tree, y_tr)\n",
        "        print(f\"Original training size: {len(y_tr)}, SMOTE resampled size: {len(y_res_tree)}\")\n",
        "\n",
        "        # Update training data for tree models\n",
        "        X_tr_tree_final, y_tr_tree_final = X_res_tree, y_res_tree\n",
        "        # For linear models, we might not use SMOTE, or use it differently.\n",
        "        # For simplicity, let's apply SMOTE only to the tree model data for now as an example.\n",
        "        X_tr_linear_final, y_tr_linear_final = X_tr_processed_linear, y_tr\n",
        "\n",
        "    else:\n",
        "        X_tr_linear_final, y_tr_linear_final = X_tr_processed_linear, y_tr\n",
        "        X_tr_tree_final, y_tr_tree_final = X_tr_processed_tree, y_tr\n",
        "\n",
        "\n",
        "    class_weight = \"balanced\" if USE_CLASS_WEIGHT_BALANCED else None\n",
        "\n",
        "    # Four models\n",
        "    models = {}\n",
        "    # Logistic Regression uses linear preproc\n",
        "    models[\"LogisticRegression\"] = Pipeline([\n",
        "        (\"clf\", LogisticRegression(\n",
        "            solver=\"saga\", penalty=\"l2\", C=1.0, max_iter=300,\n",
        "            class_weight=class_weight, random_state=RANDOM_STATE))\n",
        "    ])\n",
        "    # RandomForest uses tree preproc\n",
        "    models[\"RandomForest\"] = Pipeline([\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            n_estimators=200, n_jobs=-1, random_state=RANDOM_STATE))\n",
        "    ])\n",
        "\n",
        "    # SVM uses linear preproc, and potentially smaller subset\n",
        "    X_svm_tr, y_svm_tr = stratified_cap(\n",
        "        # Need to process SVM data separately if using SMOTE on full tree data\n",
        "        # Let's apply the appropriate preprocessor here\n",
        "        X_tr.drop(columns=[id_col], errors=\"ignore\"), y_tr, SVM_MAX_ROWS)\n",
        "\n",
        "    # Apply linear preprocessing to SVM capped data\n",
        "    X_svm_tr_processed_linear = preproc_linear.fit_transform(X_svm_tr)\n",
        "    X_svm_va_processed_linear = preproc_linear.transform(X_va.drop(columns=[id_col], errors=\"ignore\")) # Use full validation set\n",
        "\n",
        "    # If using SMOTE, decide if we apply it to the capped SVM data\n",
        "    # Let's NOT apply SMOTE to the capped SVM data for now, as SMOTE is better on larger datasets\n",
        "    X_tr_svm_final, y_tr_svm_final = X_svm_tr_processed_linear, y_svm_tr\n",
        "    X_va_svm_final, y_va_svm_final = X_svm_va_processed_linear, y_va\n",
        "\n",
        "\n",
        "    models[\"SVM\"] = Pipeline([\n",
        "        (\"clf\", SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\",\n",
        "                    probability=True, class_weight=class_weight,\n",
        "                    random_state=RANDOM_STATE))\n",
        "    ])\n",
        "\n",
        "    if xgb_available:\n",
        "        # 按训练分布设置 scale_pos_weight（提升召回）\n",
        "        # If using SMOTE, the balance changes, so scale_pos_weight should be recalculated\n",
        "        # based on the resampled data if applied to XGBoost.\n",
        "        # If SMOTE is applied to tree models, we calculate spw from the resampled data.\n",
        "        if USE_SMOTE:\n",
        "             pos_res = (y_tr_tree_final == 1).sum()\n",
        "             neg_res = (y_tr_tree_final == 0).sum()\n",
        "             spw = max((neg_res / max(pos_res, 1)), 1.0)\n",
        "             print(f\"XGBoost scale_pos_weight after SMOTE: {spw}\")\n",
        "        else:\n",
        "            pos = (y_tr == 1).sum()\n",
        "            neg = (y_tr == 0).sum()\n",
        "            spw = max((neg / max(pos, 1)), 1.0)\n",
        "            print(f\"XGBoost scale_pos_weight without SMOTE: {spw}\")\n",
        "\n",
        "\n",
        "        models[\"XGBoost\"] = Pipeline([\n",
        "            (\"clf\", XGBClassifier(\n",
        "                n_estimators=300, learning_rate=0.08, max_depth=8,\n",
        "                subsample=0.8, colsample_bytree=0.8,\n",
        "                eval_metric=\"logloss\", tree_method=\"hist\",\n",
        "                scale_pos_weight=spw,\n",
        "                n_jobs=-1, random_state=RANDOM_STATE))\n",
        "        ])\n",
        "    else:\n",
        "        print(\"XGBoost error，已跳过：\", xgb_import_error)\n",
        "\n",
        "    # 训练\n",
        "    print(\"Training & evaluating...\")\n",
        "    curves, rows, fitted = {}, [], {}\n",
        "    # Store confusion matrices for each model\n",
        "    cms = {}\n",
        "    for name, pipe in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "        # Use the correct preprocessed data for each model\n",
        "        if name == \"SVM\":\n",
        "             # SVM uses its specific capped and processed data\n",
        "             res = evaluate_model(\n",
        "                name, pipe,\n",
        "                X_tr_svm_final, y_tr_svm_final,\n",
        "                X_va_svm_final, y_va_svm_final,\n",
        "                curves\n",
        "            )\n",
        "        elif name == \"LogisticRegression\":\n",
        "             # Logistic Regression uses linear preprocessed data\n",
        "             res = evaluate_model(\n",
        "                name, pipe,\n",
        "                X_tr_linear_final, y_tr_linear_final,\n",
        "                X_va_processed_linear, y_va, # Validation data is not SMOTEd\n",
        "                curves\n",
        "            )\n",
        "        else: # RandomForest and XGBoost use tree preprocessed data (potentially SMOTEd)\n",
        "            res = evaluate_model(\n",
        "                name, pipe,\n",
        "                X_tr_tree_final, y_tr_tree_final,\n",
        "                X_va_processed_tree, y_va, # Validation data is not SMOTEd\n",
        "                curves\n",
        "            )\n",
        "        fitted[name] = res[\"pipe\"]\n",
        "        # Store the confusion matrix for this model\n",
        "        cms[name] = res[\"cm\"]\n",
        "        rows.append({k: v for k, v in res.items() if k not in (\"pipe\", \"cm\")})\n",
        "\n",
        "    metrics_df = pd.DataFrame(rows).sort_values(\"ROC_AUC\", ascending=False)\n",
        "    metrics_df.to_csv(OUT_METRICS_CSV, index=False)\n",
        "    print(\"\\nMetrics:\\n\", metrics_df)\n",
        "\n",
        "    # Print and save confusion matrix for each model\n",
        "    print(\"\\nConfusion Matrices:\")\n",
        "    for name, cm in cms.items():\n",
        "        print(f\"\\n{name}:\\n{cm}\")\n",
        "        plt.figure()\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f\"Confusion Matrix: {name}\")\n",
        "        plt.xlabel(\"Predicted\")\n",
        "        plt.ylabel(\"True\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(DATA_DIR, f\"confusion_matrix_{name}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "    # ROC 曲线\n",
        "    plt.figure()\n",
        "    for name, (fpr, tpr, auc) in curves.items():\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curves (Validation)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_ROC_PNG)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    # Prepare test data for prediction\n",
        "    test_ids = test[id_col].values\n",
        "    X_test_raw = test.drop(columns=[id_col])\n",
        "\n",
        "    # Align test data columns with training data columns before preprocessing\n",
        "    X_test_aligned = align_columns_for_inference(X_test_raw, num_cols, cat_cols)\n",
        "\n",
        "\n",
        "    # Use the appropriate preprocessor for the best model to transform test data\n",
        "    if best_name == \"SVM\" or best_name == \"LogisticRegression\":\n",
        "        X_test_processed = preproc_linear.transform(X_test_aligned)\n",
        "    else: # RandomForest, XGBoost\n",
        "        X_test_processed = preproc_tree.transform(X_test_aligned)\n",
        "\n",
        "\n",
        "    if hasattr(best_pipe, \"predict_proba\"):\n",
        "        test_proba = best_pipe.predict_proba(X_test_processed)[:, 1]\n",
        "    elif hasattr(best_pipe, \"decision_function\"):\n",
        "        dfun = best_pipe.decision_function(X_test_processed)\n",
        "        # Apply min-max scaling to decision function output\n",
        "        test_proba = (dfun - dfun.min()) / (dfun.max() - dfun.min() + 1e-9)\n",
        "    else:\n",
        "        test_proba = best_pipe.predict(X_test_processed)\n",
        "\n",
        "    # 保存提交文件\n",
        "    sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "    submission = pd.DataFrame({\n",
        "        sample.columns[0]: test_ids,\n",
        "        sample.columns[1]: test_proba\n",
        "    })\n",
        "    submission.to_csv(OUT_SUBMISSION, index=False)\n",
        "\n",
        "    import seaborn as sns\n",
        "\n",
        "    # 1. 提取模型指标\n",
        "    metrics_summary = metrics_df.set_index('model')[['Accuracy', 'Precision', 'Recall', 'F1']]\n",
        "    print(\"模型对比指标：\\n\", metrics_summary)\n",
        "\n",
        "    # 2. 绘制柱状对比图\n",
        "    metrics_summary.plot(kind='bar', figsize=(10,6))\n",
        "    plt.title('Models Performance Comparison')\n",
        "    plt.ylabel('Score')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(DATA_DIR, \"model_comparison_barplot.png\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    temp_imputer = SimpleImputer(strategy=\"median\")\n",
        "    # Need to select only numerical columns from X_cap before imputing\n",
        "    X_cap_num_only = X_cap.select_dtypes(include=np.number)\n",
        "    if id_col in X_cap_num_only.columns:\n",
        "        X_cap_num_only = X_cap_num_only.drop(columns=[id_col])\n",
        "\n",
        "    # Ensure the columns match the num_cols list used in preprocessing\n",
        "    X_cap_num_only = X_cap_num_only[num_cols]\n",
        "\n",
        "\n",
        "    X_cap_imputed_num = temp_imputer.fit_transform(X_cap_num_only)\n",
        "    X_cap_imputed_num_df = pd.DataFrame(X_cap_imputed_num, columns=num_cols, index=X_cap_num_only.index)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12,10))\n",
        "    # Calculate correlation on the imputed numerical features\n",
        "    sns.heatmap(X_cap_imputed_num_df.corr(), annot=False, cmap='coolwarm')\n",
        "    plt.title(\"Numerical Feature Correlation Heatmap (after Median Imputation)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(DATA_DIR, \"feature_correlation_heatmap.png\"))\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "    print(\"\\nArtifacts saved:\")\n",
        "    print(f\"- {OUT_METRICS_CSV}\")\n",
        "    print(f\"- {OUT_ROC_PNG}\")\n",
        "    print(f\"- {OUT_CM_PNG}\")\n",
        "    print(f\"- {OUT_SUBMISSION}\")\n",
        "    print(f\"- {os.path.join(DATA_DIR, 'model_comparison_barplot.png')}\")\n",
        "    print(f\"- {os.path.join(DATA_DIR, 'feature_correlation_heatmap.png')}\")\n",
        "    # List saved confusion matrix files\n",
        "    for name in cms.keys():\n",
        "        print(f\"- {os.path.join(DATA_DIR, f'confusion_matrix_{name}.png')}\")\n",
        "\n",
        "\n",
        "    print(f\"Done in {time.time()-t0:.1f}s\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading & merging...\n",
            "Applying SMOTE...\n",
            "Applying SMOTE to preprocessed tree data...\n",
            "Original training size: 85434, SMOTE resampled size: 166576\n",
            "XGBoost scale_pos_weight after SMOTE: 1.0\n",
            "Training & evaluating...\n",
            "Training LogisticRegression...\n",
            "Training RandomForest...\n",
            "Training SVM...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_ids = test[id_col].values\n",
        "X_test_raw = test.drop(columns=[id_col])\n",
        "\n",
        "# Align test data columns with training data columns before preprocessing\n",
        "X_test_aligned = align_columns_for_inference(X_test_raw, num_cols, cat_cols)\n",
        "\n",
        "\n",
        "# Use the appropriate preprocessor for the best model to transform test data\n",
        "if best_name == \"SVM\" or best_name == \"LogisticRegression\":\n",
        "    X_test_processed = preproc_linear.transform(X_test_aligned)\n",
        "else: # RandomForest, XGBoost\n",
        "    X_test_processed = preproc_tree.transform(X_test_aligned)"
      ],
      "metadata": {
        "id": "78eyKyHVOg-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "欢迎使用 Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}