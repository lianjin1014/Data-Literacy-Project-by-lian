{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lianjin1014/Data-Literacy-Project-by-lian/blob/main/%E6%AC%A2%E8%BF%8E%E4%BD%BF%E7%94%A8_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, time, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (roc_auc_score, accuracy_score, precision_score,\n",
        "                             recall_score, f1_score, confusion_matrix, roc_curve)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DATA_DIR = \".\"\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE    = 0.2\n",
        "\n",
        "MAX_ROWS     = 120_000    # LR / RF / XGB 的训练上限\n",
        "SVM_MAX_ROWS = 25_000     # SVM 更耗时，用更小上限\n",
        "\n",
        "# 是否对线性/核方法启用类不平衡权重\n",
        "USE_CLASS_WEIGHT_BALANCED = True"
      ],
      "metadata": {
        "id": "Km7A8p-oDUGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------- 路径 --------------------\n",
        "TRAIN_TRANS_PATH = os.path.join(DATA_DIR, \"train_transaction.csv\")\n",
        "TRAIN_ID_PATH    = os.path.join(DATA_DIR, \"train_identity.csv\")\n",
        "TEST_TRANS_PATH  = os.path.join(DATA_DIR, \"test_transaction.csv\")\n",
        "TEST_ID_PATH     = os.path.join(DATA_DIR, \"test_identity.csv\")\n",
        "SAMPLE_SUB_PATH  = os.path.join(DATA_DIR, \"sample_submission.csv\")\n",
        "\n",
        "# -------------------- 输出 --------------------\n",
        "OUT_METRICS_CSV  = os.path.join(DATA_DIR, \"model_metrics.csv\")\n",
        "OUT_ROC_PNG      = os.path.join(DATA_DIR, \"roc_curves.png\")\n",
        "OUT_CM_PNG       = os.path.join(DATA_DIR, \"confusion_matrix_best.png\")\n",
        "OUT_SUBMISSION   = os.path.join(DATA_DIR, \"submission_best_model.csv\")\n"
      ],
      "metadata": {
        "id": "o7Nr67_4DUDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb_available = True\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "except Exception as e:\n",
        "    xgb_available = False\n",
        "    xgb_import_error = str(e)\n",
        "\n",
        "def load_and_merge(trans_path, id_path):\n",
        "    trans = pd.read_csv(trans_path)\n",
        "    ident = pd.read_csv(id_path)\n",
        "    return trans.merge(ident, on=\"TransactionID\", how=\"left\")\n",
        "\n",
        "def stratified_cap(X, y, cap, seed=RANDOM_STATE):\n",
        "    if (cap is None) or (len(X) <= cap):\n",
        "        return X, y\n",
        "    Xs, _, ys, _ = train_test_split(X, y, train_size=cap, stratify=y, random_state=seed)\n",
        "    return Xs, ys\n",
        "\n",
        "def align_columns_for_inference(X_df, num_cols, cat_cols):\n",
        "    X = X_df.copy()\n",
        "    wanted = list(num_cols) + list(cat_cols)\n",
        "\n",
        "    # add missing\n",
        "    for c in wanted:\n",
        "        if c not in X.columns:\n",
        "            if c in num_cols:\n",
        "                X[c] = np.nan\n",
        "            else:\n",
        "                X[c] = pd.Series([np.nan] * len(X), dtype=\"object\")\n",
        "\n",
        "    # drop extras & reorder\n",
        "    X = X[wanted]\n",
        "    return X\n",
        "\n",
        "def evaluate_model(name, pipe, Xtr, ytr, Xva, yva, store_curves):\n",
        "    pipe.fit(Xtr, ytr)\n",
        "    if hasattr(pipe, \"predict_proba\"):\n",
        "        proba = pipe.predict_proba(Xva)[:, 1]\n",
        "    elif hasattr(pipe, \"decision_function\"):\n",
        "        dfun  = pipe.decision_function(Xva)\n",
        "        proba = (dfun - dfun.min()) / (dfun.max() - dfun.min() + 1e-9)\n",
        "    else:\n",
        "        proba = pipe.predict(Xva)\n",
        "\n",
        "    preds = (proba >= 0.5).astype(int)\n",
        "\n",
        "    auc  = roc_auc_score(yva, proba)\n",
        "    acc  = accuracy_score(yva, preds)\n",
        "    prec = precision_score(yva, preds, zero_division=0)\n",
        "    rec  = recall_score(yva, preds, zero_division=0)\n",
        "    f1   = f1_score(yva, preds, zero_division=0)\n",
        "    cm   = confusion_matrix(yva, preds)\n",
        "    fpr, tpr, _ = roc_curve(yva, proba)\n",
        "    store_curves[name] = (fpr, tpr, auc)\n",
        "\n",
        "    return {\"model\": name, \"ROC_AUC\": auc, \"Accuracy\": acc,\n",
        "            \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"cm\": cm, \"pipe\": pipe}\n"
      ],
      "metadata": {
        "id": "-11u_3yzDUAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    t0 = time.time()\n",
        "    print(\"Loading & merging...\")\n",
        "    train = load_and_merge(TRAIN_TRANS_PATH, TRAIN_ID_PATH)\n",
        "    test  = load_and_merge(TEST_TRANS_PATH, TEST_ID_PATH)\n",
        "\n",
        "    assert \"isFraud\" in train.columns, \"训练集必须包含目标列 isFraud\"\n",
        "    id_col = \"TransactionID\"\n",
        "\n",
        "    y_full = train[\"isFraud\"].astype(int)\n",
        "    X_full = train.drop(columns=[\"isFraud\"])\n",
        "    assert id_col in X_full.columns and id_col in test.columns\n",
        "\n",
        "    # 分层抽样（整体）\n",
        "    X_cap, y_cap = stratified_cap(X_full, y_full, MAX_ROWS)\n",
        "\n",
        "    # 训练/验证切分\n",
        "    X_tr, X_va, y_tr, y_va = train_test_split(\n",
        "        X_cap, y_cap, test_size=TEST_SIZE, stratify=y_cap, random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # 列类型识别\n",
        "    num_cols = X_tr.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if id_col in num_cols: num_cols.remove(id_col)\n",
        "    cat_cols = [c for c in X_tr.columns if c not in num_cols and c != id_col]\n",
        "\n",
        "    # 预处理：线性模型用标准化；树模型不用标准化\n",
        "    num_imputer_scaler = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ])\n",
        "    num_imputer_only = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    ])\n",
        "    cat_ordinal = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"enc\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
        "    ])\n",
        "    preproc_linear = ColumnTransformer([\n",
        "        (\"num\", num_imputer_scaler, num_cols),\n",
        "        (\"cat\", cat_ordinal,       cat_cols),\n",
        "    ], remainder=\"drop\")\n",
        "    preproc_tree = ColumnTransformer([\n",
        "        (\"num\", num_imputer_only,  num_cols),\n",
        "        (\"cat\", cat_ordinal,       cat_cols),\n",
        "    ], remainder=\"drop\")\n",
        "\n",
        "    class_weight = \"balanced\" if USE_CLASS_WEIGHT_BALANCED else None\n",
        "\n",
        "    # 四模型\n",
        "    models = {}\n",
        "    models[\"LogisticRegression\"] = Pipeline([\n",
        "        (\"prep\", preproc_linear),\n",
        "        (\"clf\", LogisticRegression(\n",
        "            solver=\"saga\", penalty=\"l2\", C=1.0, max_iter=300,\n",
        "            class_weight=class_weight, random_state=RANDOM_STATE))\n",
        "    ])\n",
        "    models[\"RandomForest\"] = Pipeline([\n",
        "        (\"prep\", preproc_tree),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            n_estimators=200, n_jobs=-1, random_state=RANDOM_STATE))\n",
        "    ])\n",
        "\n",
        "    X_svm_tr, y_svm_tr = stratified_cap(\n",
        "        X_tr.drop(columns=[id_col], errors=\"ignore\"), y_tr, SVM_MAX_ROWS)\n",
        "    X_svm_va, y_svm_va = X_va.drop(columns=[id_col], errors=\"ignore\"), y_va\n",
        "\n",
        "    models[\"SVM\"] = Pipeline([\n",
        "        (\"prep\", preproc_linear),\n",
        "        (\"clf\", SVC(kernel=\"rbf\", C=1.0, gamma=\"scale\",\n",
        "                    probability=True, class_weight=class_weight,\n",
        "                    random_state=RANDOM_STATE))\n",
        "    ])\n",
        "\n",
        "    if xgb_available:\n",
        "        # 按训练分布设置 scale_pos_weight（提升召回）\n",
        "        pos = (y_tr == 1).sum()\n",
        "        neg = (y_tr == 0).sum()\n",
        "        spw = max((neg / max(pos, 1)), 1.0)\n",
        "\n",
        "        models[\"XGBoost\"] = Pipeline([\n",
        "            (\"prep\", preproc_tree),\n",
        "            (\"clf\", XGBClassifier(\n",
        "                n_estimators=300, learning_rate=0.08, max_depth=8,\n",
        "                subsample=0.8, colsample_bytree=0.8,\n",
        "                eval_metric=\"logloss\", tree_method=\"hist\",\n",
        "                scale_pos_weight=spw,\n",
        "                n_jobs=-1, random_state=RANDOM_STATE))\n",
        "        ])\n",
        "    else:\n",
        "        print(\"XGBoost 未安装，已跳过：\", xgb_import_error)\n",
        "\n",
        "    # 训练\n",
        "    print(\"Training & evaluating...\")\n",
        "    curves, rows, fitted = {}, [], {}\n",
        "    for name, pipe in models.items():\n",
        "        if name == \"SVM\":\n",
        "            res = evaluate_model(\n",
        "                name, pipe,\n",
        "                X_svm_tr, y_svm_tr,\n",
        "                X_svm_va, y_svm_va,\n",
        "                curves\n",
        "            )\n",
        "        else:\n",
        "            res = evaluate_model(\n",
        "                name, pipe,\n",
        "                X_tr.drop(columns=[id_col], errors=\"ignore\"), y_tr,\n",
        "                X_va.drop(columns=[id_col], errors=\"ignore\"), y_va,\n",
        "                curves\n",
        "            )\n",
        "        fitted[name] = res[\"pipe\"]\n",
        "        rows.append({k: v for k, v in res.items() if k not in (\"pipe\", \"cm\")})\n",
        "\n",
        "    metrics_df = pd.DataFrame(rows).sort_values(\"ROC_AUC\", ascending=False)\n",
        "    metrics_df.to_csv(OUT_METRICS_CSV, index=False)\n",
        "    print(\"\\nMetrics:\\n\", metrics_df)\n",
        "\n",
        "    # ROC 曲线\n",
        "    plt.figure()\n",
        "    for name, (fpr, tpr, auc) in curves.items():\n",
        "        plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.3f})\")\n",
        "    plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curves (Validation)\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_ROC_PNG)\n",
        "    plt.close()\n",
        "\n",
        "    # 最近模型的混淆矩阵\n",
        "    best_name = metrics_df.iloc[0][\"model\"]\n",
        "    best_pipe = fitted[best_name]\n",
        "\n",
        "    X_cap_features = X_cap.drop(columns=[id_col], errors=\"ignore\")\n",
        "\n",
        "    X_cap_aligned = align_columns_for_inference(X_cap_features, num_cols, cat_cols)\n",
        "\n",
        "    print(f\"\\nBest model by AUC: {best_name}  Retraining on capped full train...\")\n",
        "    best_pipe.fit(X_cap_aligned, y_cap)\n",
        "\n",
        "    if best_name == \"SVM\":\n",
        "        Xv = align_columns_for_inference(X_svm_va, num_cols, cat_cols)\n",
        "        yv = y_svm_va\n",
        "    else:\n",
        "        Xv = align_columns_for_inference(X_va.drop(columns=[id_col], errors=\"ignore\"), num_cols, cat_cols)\n",
        "        yv = y_va\n",
        "\n",
        "    if hasattr(best_pipe, \"predict_proba\"):\n",
        "        proba_best = best_pipe.predict_proba(Xv)[:, 1]\n",
        "    elif hasattr(best_pipe, \"decision_function\"):\n",
        "        dfun = best_pipe.decision_function(Xv)\n",
        "        proba_best = (dfun - dfun.min()) / (dfun.max() - dfun.min() + 1e-9)\n",
        "    else:\n",
        "        proba_best = best_pipe.predict(Xv)\n",
        "    preds_best = (proba_best >= 0.5).astype(int)\n",
        "    cm_best = confusion_matrix(yv, preds_best)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.imshow(cm_best, interpolation=\"nearest\")\n",
        "    plt.title(f\"Confusion Matrix: {best_name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    for (i, j), z in np.ndenumerate(cm_best):\n",
        "        plt.text(j, i, f\"{z}\", ha=\"center\", va=\"center\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_CM_PNG)\n",
        "    plt.close()\n",
        "\n",
        "    test_ids = test[id_col].values\n",
        "    X_test_raw = test.drop(columns=[id_col])\n",
        "    X_test     = align_columns_for_inference(X_test_raw, num_cols, cat_cols)\n",
        "\n",
        "    if hasattr(best_pipe, \"predict_proba\"):\n",
        "        test_proba = best_pipe.predict_proba(X_test)[:, 1]\n",
        "    elif hasattr(best_pipe, \"decision_function\"):\n",
        "        dfun = best_pipe.decision_function(X_test)\n",
        "        test_proba = (dfun - dfun.min()) / (dfun.max() - dfun.min() + 1e-9)\n",
        "    else:\n",
        "        test_proba = best_pipe.predict(X_test)\n",
        "\n",
        "    # 保存提交文件\n",
        "    sample = pd.read_csv(SAMPLE_SUB_PATH)\n",
        "    submission = pd.DataFrame({\n",
        "        sample.columns[0]: test_ids,\n",
        "        sample.columns[1]: test_proba\n",
        "    })\n",
        "    submission.to_csv(OUT_SUBMISSION, index=False)\n",
        "    import seaborn as sns\n",
        "\n",
        "    # 1. 提取模型指标\n",
        "    metrics_summary = metrics_df.set_index('model')[['Accuracy', 'Precision', 'Recall', 'F1']]\n",
        "    print(\"模型对比指标：\\n\", metrics_summary)\n",
        "\n",
        "    # 2. 绘制柱状对比图\n",
        "    metrics_summary.plot(kind='bar', figsize=(10,6))\n",
        "    plt.title('Models Performance Comparison')\n",
        "    plt.ylabel('Score')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(DATA_DIR, \"model_comparison_barplot.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 3. 每个模型的混淆矩阵（已在之前代码中实现，现在可以保存或显示）\n",
        "\n",
        "    # (可选) 4. 相关性热图\n",
        "    # 计算全部特征的相关性\n",
        "    full_features = X_full.drop(columns=[id_col])\n",
        "    plt.figure(figsize=(12,10))\n",
        "    sns.heatmap(full_features.corr(), annot=False, cmap='coolwarm')\n",
        "    plt.title(\"Feature Correlation Heatmap\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(DATA_DIR, \"feature_correlation_heatmap.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nArtifacts saved:\")\n",
        "    print(f\"- {OUT_METRICS_CSV}\")\n",
        "    print(f\"- {OUT_ROC_PNG}\")\n",
        "    print(f\"- {OUT_CM_PNG}\")\n",
        "    print(f\"- {OUT_SUBMISSION}\")\n",
        "    print(f\"Done in {time.time()-t0:.1f}s\")\n"
      ],
      "metadata": {
        "id": "EWQIODkaDT9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "mpMbiQuVDT6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_d9XXJWnDT3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "欢迎使用 Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}